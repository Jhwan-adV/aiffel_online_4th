{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6. 신경망과 함수  \n",
    "자극과 반응 사이. 신경망에서 일어나는 일은 수학의 함수와 같다.  \n",
    "\n",
    "다음 함수를 표현해보자.\n",
    "$$f(x) = x^2 + e^{x+1}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "428.428793492735"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math as m\n",
    "\n",
    "def f(x):\n",
    "    return x ** 2 + m.exp(x+1)\n",
    "\n",
    "f(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6.1. 함수의 역할"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Relation = x와 y의 관계를 나타내는 도구**\n",
    "\n",
    "    > y는 x의 함수이다\n",
    "\n",
    "    이 선언이 내포하는 내용:\n",
    "    - y는 x의 변화에 종속적이다.  \n",
    "    - x의 변화 정도에 따른 y의 변화는 함수의 형태에 따라 결정된다.  \n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Transformation = x를 변환하는 도구**  \n",
    "\n",
    "    [Q11] Linear Transformation 과 matrix $\\times$ vector multification 이 연결되어 설명  \n",
    "\n",
    "    [Q12] Transformation과 Function이 연결되어 설명. 선형대수학에서는 벡터를 입력바당 벡터를 출력한다.  \n",
    "\n",
    "    [Q13-14] Transformation은 좌표축을 변환해 2차원 평면을 변환하는 효과를 만들어낸다. 이 때 두 가지 조건을 지켜야 한다.  \n",
    "    - 원점은 움직일 수 없다.  \n",
    "    - 모든 선은 휘어질 수 없다. (각 좌표 간을 잇는 선 또한 마찬가지)  \n",
    "        - 모든 선은 평행이고, 같은 간격으로 배치되어야 한다.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 변환하는 함수에 벡터 넣어보기\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def linear_transformation(X, A):\n",
    "    transformed_vector = np.matmul(X, A)\n",
    "    return transformed_vector\n",
    "\n",
    "vector_x = np.array([-1,2])\n",
    "matrix_A = np.array([[1, -2], [3,0]])\n",
    "\n",
    "linear_transformation(vector_x, matrix_A)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Mapping: x의 공간에서 y의 공간으로**\n",
    "\n",
    "    함수의 입력이 벡터인지, 스칼라인지에 따라 구분\n",
    "    - One-to-One Mapping: 입력값과 출력값 모두 스칼라인 경우. 값 하나를 입력했을 때 하나의 값만 나오게 된다.  \n",
    "    - Many-to-One Mapping: 벡터를 입력받아 스칼라를 출력하는 경우. 값 여러 개를 입력하여 하나의 값을 반환받는다. \n",
    "    - Many-to-Many Mapping: 입력값도, 출력값도 벡터인 경우.  \n",
    "\n",
    "    출력이 One인 Mapping은 회귀에 많이 사용되고, 출력이 Many인 Mapping은 분류에 많이 사용된다.  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6.2. 함수와 모델\n",
    "\n",
    "#### 함수와 모델의 차이점  \n",
    "\n",
    "함수는 형태가 고정된 경우가 대부분으로, 사용하고자 하는 함수를 찾아내기만 하면 된다.  \n",
    "하지만 모델은 함수이더라도 여러 답이 존재하며, 어떤 형태로 나타날 지 모른다.  \n",
    "그래서 모델은 '완벽한 함수'를 찾는 수학 문제가 아닌, '원하는 답에 최대한 가까운 함수'를 찾는 시도에 가깝다.  \n",
    "\n",
    "#### 더 나은 함수를 찾아가는 과정  \n",
    "\n",
    "비정형 데이터의 경우, 계층적인 구조가 데이터를 단계적으로 표현할 수 있기 때문에 대부분 신경망을 사용한다.  \n",
    "\n",
    "1. **Inductive Bias (or prior) 설정**  \n",
    "    데이터를 설명할 수 있는 최적의 함수가 특정한 함수 공간에 존재할 것이라는 가설  \n",
    "    이 가설에 따라 결과가 완전히 달라질 수 있다.  \n",
    "    모델이 전체 데이터를 볼 수는 없기 때문에, 일반화를 더 쉽게 하기 위해 일반적인 패턴을 잘 반영해야 하는 것.  \n",
    "    도메인 지식을 가진 사람이 알맞은 함수 형태로 모델을 선택하며, 이를 위해 Inductive Bias가 필요한 것이다.  \n",
    "\n",
    "2. **해당 공간 안에서 최적의 함수를 찾아나간다**  \n",
    "    머신러닝/딥러닝에서의 '모델 학습'으로, 성능이 좋지 못하던 모델을 개선하는 과정이다"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7. AI의 현재와 미래 - 머신러닝과 딥러닝  \n",
    "머신러닝과 딥러닝이 어떤 차이를 갖는지, 추구하는 것이 어떻게 다른가?  \n",
    "\n",
    "- 머신러닝: 패턴을 학습해 필요한 값을 예측하는 과정  \n",
    "- 딥러닝: 데이터에 내재된 표현 자체를 학습하는 과정  \n",
    "\n",
    "머신러닝은 정형 데이터 처리를 위해 설계되어, 사람이 Feature를 가공해줘야 한다. (Feature Engineering)  \n",
    "딥러닝은 모델을 더 복잡하게 해 사람의 개입을 최소화하며, 뚜렷한 형태가 없는 데이터에서 표현을 추출하는 데 강하다.  (End-to-End Learning)  \n",
    "\n",
    "\n",
    "#### **머신러닝 1**  \n",
    "(https://www.stechstar.com/user/zbxe/index.php?mid=study_SQL&page=14&document_srl=63467)\n",
    "\n",
    "- **아이의 학습 과정** 개념의 예시를 경험하면서 그 특징을 머릿속에서 좁혀나가는 과정\n",
    "\n",
    "- **선형 모델** 대상을 구분하기 위해, 직선(평면 혹은 초평면)으로 정의하는 러닝 모델  \n",
    "\n",
    "- **이미지 인식에 있어서 학습모델** 이미지 데이터는 데이터 자체가 복잡해 선형 모델의 활용 가능성이 매우 낮은 편. 이를 해소하기 위해 인공신경망 등 더 복잡한 모델을 활용함  \n",
    "\n",
    "\n",
    "#### **딥러닝 1**  \n",
    "(https://www.stechstar.com/user/zbxe/index.php?mid=study_SQL&page=13&document_srl=63482)\n",
    "\n",
    "- **퍼셉트론** 입력 벡터의 각 성분에 가중치를 곱하여 합산한 뒤 활성함수를 적용한 출력값. 돌기로 주변 자극을 받아들여 핵으로 전달한 뒤 다른 곳으로 전달하는 **뉴런**과 비슷하여 인공뉴런(artificial neuron)이라고도 함\n",
    "\n",
    "- **다층 퍼셉트론** 입력층, 출력층, 은닉층 등이 2차원적으로 연결되는 인공신경망의 일종. 층(Layer) 개수를 셀 때 은닉층은 포함되지 않는다.  \n",
    "\n",
    "- **심층 신경망** 은닉층의 개수가 많아 *깊어진* 인공신경망\n",
    "\n",
    "- **대표적인 딥러닝 모델 세 가지**\n",
    "    1. **완전 연결 신경망 (=다층 퍼셉트론)** \n",
    "\n",
    "        같은 층에 존재하는 노드끼리는 연결되지 않고, 인접한 층에 위치한 노드 간에 연결 관계가 존재한다.  \n",
    "    \n",
    "    2. **컨볼루션 신경망**  \n",
    "\n",
    "        이미지 데이터는 픽셀 간의 위치 관련성, 유사성, 다시 말해 공간 정보가 중요한 데이터다.  \n",
    "        다중 퍼셉트론 적용을 위해 1차원 텐서로 변환하면 이 정보가 손실되고, 가중치 또한 기하급수적으로 증가한다.  \n",
    "        이를 극복하기 위해 **커널**을 설정, 겹치는 부분에만 가중치를 적용한 **특성 맵**을 출력하는 신경망이 컨볼루션 신경망이다.  \n",
    "    \n",
    "        - 채널: 색 성분으로 흑백의 경우 1개, 일반적인 RGB이미지는 3개이다.  \n",
    "        - 커널(필터): 이미지의 특징을 알아내기 위한 필터. 커널과 이미지가 Mapping되는 부분에 대해서만 합성곱을 진행한다.  \n",
    "        - 특성 맵(Feature map): 합성곱의 결과로 나온 이미지 특성\n",
    "        - 스트라이드: 커널의 이동 범위 (1이면 1칸씩, 2면 2칸씩 이동하며 특성 맵을 만든다.)\n",
    "        - 패딩: 입력 이미지의 크기와 특성 맵의 크기를 맞추기 위해 일정 수만큼 행/열을 추가함\n",
    "        - 풀링: 출력된 Feature map을 downsampling하여 크기를 줄임  \n",
    "    <br>    \n",
    "        \n",
    "    3. **순환 신경망(Recurrent Neural Network)**\n",
    "\n",
    "        시퀀스(sequence) : 첫 데이터가 다음 데이터를 결정하는, 데이터 순서가 중요한 데이터  \n",
    "        시퀀스의 가변성과 선후관계를 파악하기 위해 출현한 신경망으로, 아래 아이디어를 구현했다.  \n",
    "\n",
    "        - 시퀀스 상의 원소를 매 시점에 하나씩 입력한다  \n",
    "        - 특정 시점의 은닉층 출력 벡터를 시퀀스 바로 다음의 원소와 같이 입력한다.  \n",
    "        - 자연어 처리 분야에 많이 적용된다.  \n",
    "\n",
    "\n",
    "\n",
    "#### **딥러닝 2**\n",
    "(https://www.stechstar.com/user/zbxe/index.php?mid=study_SQL&page=13&document_srl=63499)\n",
    "\n",
    "- **딥러닝의 강점** \n",
    "\n",
    "    머신러닝은 Feature Extraction 작업이 필요했다. 이 부분에 있어서는 전문가의 통찰이 중요했다.  \n",
    "    특히 선형 모델과 같이 표현이 얕은 모델은, 사람이 가중치를 찾아줘야만 했다.  \n",
    "    하지만 딥러닝은, 원본 데이터에서 최적의 성능을 발휘하는 요인 표현 방법을 스스로 학습한다.  \n",
    "\n",
    "- **딥러닝 기술이 줄인 노력**  \n",
    "\n",
    "    - 제품 자동화에 적용하기 위해 거듭해야 하는 관찰과 실험  \n",
    "    - 제품 외부 상황, 내재적 속성 변화에 따라 추가적으로 들어가는 비용  \n",
    "    - 어려운 머신러닝 문제들에 대한 해결 성능이 비약적으로 상승함  \n",
    "<br>\n",
    "\n",
    "- **CNN의 낮은 층과 높은 층 각각이 추출하는 특징**  \n",
    "\n",
    "    대체로 은닉층의 개수가 증가할수록, 요인 표현 방법을 더욱 효과적으로 학습한다.    \n",
    "    낮은 층 = 이미지 세부 영역에서의 경계, 명암, 색상 변화의 (low-level) 특징을 요인 표현으로 포착  \n",
    "    높은 층 = 가까운 것끼리 조합, 넓은 영역에서 고수준의 특성을 요인 표현으로 포착\n",
    "\n",
    "\n",
    "- **딥러닝의 약점**  \n",
    "\n",
    "    \"데이터를 먹는 괴물\" - 인간 수준에 도달하기 위해 엄청난 양의 데이터를 필요로 함  \n",
    "    최소한의 데이터를 통해 준수한 성능을 내는 딥러닝 기술 개발이 필요함. 아직은 흙길(2019년 기준)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8. AI의 현재와 미래 - 설명 가능한 인공지능 (XAI) - 키워드 각각은 날 잡고 공부\n",
    "(https://realblack0.github.io/2020/04/27/explainable-ai.html)\n",
    "\n",
    "### XAI의 구성 요소\n",
    "\n",
    "#### Complexity\n",
    "- intrinsic\n",
    "- post-hoc\n",
    "\n",
    "#### Scope\n",
    "- global\n",
    "- local\n",
    "    - LIME(Local Interpretable Model-Agnostic Explanation)\n",
    "\n",
    "#### Dependency (model-specific / model agnostic)\n",
    "- model-specific\n",
    "    - activation\n",
    "    - weight viz\n",
    "    - maximization optimization\n",
    "    - attribution\n",
    "    - deconvolution\n",
    "    - CAM(Class Activation Map)\n",
    "    - Grad-CAM (Gradient Class Activation Map)  \n",
    "<br>\n",
    "\n",
    "- model-agnostic\n",
    "    - surrogate\n",
    "    - sensitivity analysis\n",
    "    - partial dependence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

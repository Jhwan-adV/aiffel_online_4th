# Exploration 4. 강화학습

## Intro
paired data가 없을 때 비지도학습(unsupervised learning)을 사용해야 할까?  

오히려, 처음 떠올려야 할 것은 강화학습(reinforcement learning)이다!  

정답을 알고 있다면 지도학습을 사용해야겠지만, 사람이 생각한 정답보다 더 최적의 답을 찾아낼 수도 있기 때문에 강화학습을 사용하게 된다.  
___

## Reinforcement Learning

State -> 강화학습 함수 -> action -> reward의 반복.  
강화학습은 이 개념을 명확히해아 한다.  
모델을 설계하기 전에 어떤 State, action, reward를 줄 것인지 정하기

### 슈퍼마리오 예제
**[먼저 State, 함수, action을 찾아봅시다]**  

**내 대답**  
- State - 캐릭터 위치, 몬스터, 수수께끼 블록, 벽돌, 바닥 구멍의 위치     
- 함수 - 캐릭터와 각 대상 간의 거리, 얼마나 앞으로 진행했는지 측정  
- Action - 4방향키, 점프와 가속, (완전체) 불꽃 공격  

**실제 학습에서는**  
- State - 게임 코드가 있다면 활용, 아니라면 플레이 영상 (개발자에 따라 다르지만, 많은 정보를 입력할수록 좋은 편)  
- 함수 - Reward를 **깃발**로 할지, **획득한 코인 갯수**로 할지 정한다.  
- Action - 동일함

### 자율주행자동차 예제
입구가 둘, 출구가 둘인 곳. 병목 현상이 나타나는 구간에서 안전하게 목적지로 나가려면 어떻게 State, Action, Reward를 잡아야 할까?  
- State: 차간 거리, 분기점까지의 거리, 차선 정보
- Action: 차선 변경, 브레이크, 엑셀, 좌(혹은 우)회전 깜빡이, 핸들 꺾은 정도
- Reward: 충돌은 Panelty, 목적지로 가면 Reward, 최소한의 속도도 Reward 

### 양궁로봇 예제
양궁 선수로서 잘 하는 방법을 고민하는 과정과 비슷 

- state - 과녁까지의 거리, 풍향과 풍속
- action - 활시위 당기는 힘, 방향조정
- reward - 적중한 점수에 따라 Reward, 과녁을 빗나가면 Panelty

## 강화학습의 활용 

한 번도 보지 못한 State가 나오면 강화학습이 어렵다.  
Reward가 생명에 중대한 영향을 미치는 경우에는 활용하기 어려울 수 있다.  

#### 응용분야  
Thumbnail Selection, 내비게이션 경로추천


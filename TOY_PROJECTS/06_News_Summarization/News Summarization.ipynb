{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "375cb3af",
   "metadata": {},
   "source": [
    "# TOY PROJECT 06 - News Summarization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b21d19b7",
   "metadata": {},
   "source": [
    "## 1. Library and File Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63a47d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Using cached bs4-0.0.1.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Using cached beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Downloading soupsieve-2.4.1-py3-none-any.whl (36 kB)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1266 sha256=e954485f82f9f6728a5275a2182b4f184c4190163b3f8fa97270c97c8f93a6ca\n",
      "  Stored in directory: c:\\users\\kjaeh\\appdata\\local\\pip\\cache\\wheels\\25\\42\\45\\b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
      "Successfully built bs4\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.12.2 bs4-0.0.1 soupsieve-2.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\000_git\\040_aiffel\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\000_git\\040_aiffel\\.venv\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "! pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71988c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kjaeh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bs4'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mstopwords\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m stopwords\n\u001b[1;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbs4\u001b[39;00m \u001b[39mimport\u001b[39;00m BeautifulSoup \n\u001b[0;32m     14\u001b[0m \u001b[39m# Building Model\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m Tokenizer \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bs4'"
     ]
    }
   ],
   "source": [
    "# Data Handling and Visualization\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Text Preprocessing\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "# Building Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a28f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 98401 entries, 0 to 98400\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   headlines  98401 non-null  object\n",
      " 1   text       98401 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# Download the data\n",
    "'''\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/sunnysai12345/\" \\\n",
    "                           \"News_Summary/master/news_summary_more.csv\", \\\n",
    "                           filename=\"news_summary_more.csv\")\n",
    "'''\n",
    "\n",
    "data = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773385c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline Sample: upGrad learner switches to career in ML & Al with 90% salary hike\n",
      "\n",
      "Text Sample: Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.\n"
     ]
    }
   ],
   "source": [
    "# See some sample text & headlines\n",
    "print(f\"Headline Sample: {data['headlines'][0]}\\n\")\n",
    "print(f\"Text Sample: {data['text'][0]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db6d421d",
   "metadata": {},
   "source": [
    "## 2. Preprocessing for Abstract Summarization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb15a290",
   "metadata": {},
   "source": [
    "### 2.1. Drop Overlapped Data & Impute Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc792be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Length (Initial): 98401\n",
      "Data Length (Dropped_Duplicates): 98379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "headlines    0\n",
       "text         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original Dataset\n",
    "print(f\"Data Length (Initial): {len(data)}\")\n",
    "\n",
    "# Drop Overlapped data \n",
    "data.drop_duplicates(inplace=True)\n",
    "print(f\"Data Length (Dropped_Duplicates): {len(data)}\")\n",
    "\n",
    "# Impute Missing Values - No missing value\n",
    "data.isnull().sum()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a254976",
   "metadata": {},
   "source": [
    "### 2.2. Normalization, Stopwords, Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd0b37b0",
   "metadata": {},
   "source": [
    "#### 2.2.1. Contractions & Stopwords for Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25df72c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Contractions:  120 \n",
      "\n",
      "Stopwords Example: ['i', 'me', 'my', 'myself', 'we']\n",
      "Number of Stopwords: 179\n"
     ]
    }
   ],
   "source": [
    "# Conractions\n",
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "print(\"Number of Contractions: \", len(contractions), \"\\n\")\n",
    "\n",
    "\n",
    "# Stopwords (from NLTK corpus)\n",
    "list_stopwords = stopwords.words('english')\n",
    "\n",
    "print(f\"Stopwords Example: {list_stopwords[:5]}\")\n",
    "print(f\"Number of Stopwords: {len(list_stopwords)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "004c80b0",
   "metadata": {},
   "source": [
    "#### 2.2.2. Processing text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66b25d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function - Processing text data\n",
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    \n",
    "    # Lowercase all strings\n",
    "    sentence = sentence.lower() # Make all string lowercase    \n",
    "    \n",
    "    # Get rid of all HTML tags (ex: <br>, <a>, ...etc.)\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text \n",
    "    \n",
    "    # Normalize all contractions\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) \n",
    "    \n",
    "    # Other \n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # Remove parenthesized phrases\n",
    "    sentence = re.sub('\"','', sentence) # Remove double quotation marks (\"\")\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # Replace special characters and numbers with a space\n",
    "    sentence = re.sub(r\"'s\\b\",\"\", sentence) # Remove possessives (ex roland's -> roland)\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence) # contract too much 'm's (ex. ummmmmmm yeah -> umm yeah)\n",
    "    \n",
    "    # remove_stopwords = True\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in list_stopwords if len(word) > 1)\n",
    "    \n",
    "    # remove_stopwords = False (for headlines)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bada2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  everything bought great infact ordered twice third ordered wasfor mother father \n",
      "\n",
      "headline (stopwords included): is not it my great way to start the day\n",
      "headline (stopwords excluded): great way start day\n"
     ]
    }
   ],
   "source": [
    "# Test the processing function\n",
    "temp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.'\n",
    "temp_summary = 'Isn\\'t it my great way to start (or finish) the day?'\n",
    "\n",
    "print(\"text: \", preprocess_sentence(temp_text), \"\\n\")\n",
    "print(\"headline (stopwords included):\", preprocess_sentence(temp_summary, False)) # Headline - stopwords included\n",
    "print(\"headline (stopwords excluded):\", preprocess_sentence(temp_summary, True)) # Headline - stopwords excluded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9a042a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers. \n",
      "\n",
      "Preprocessed text:  saurav kant alumnus upgrad iiit pg program machine learning artificial intelligence sr systems engineer infosys almost years work experience program upgrad degree career support helped transition data scientist tech mahindra salary hike upgrad online power learning powered lakh careers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98379"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process data['text'] into clean_text\n",
    "clean_text = []\n",
    "for text in data['text']:\n",
    "    preprocessed_text = preprocess_sentence(text)\n",
    "    clean_text.append(preprocessed_text)\n",
    "\n",
    "# Check it out the result\n",
    "print(\"Original Text: \", data['text'][0], \"\\n\")\n",
    "print(\"Preprocessed text: \", clean_text[0])\n",
    "\n",
    "len(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c7e61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original headline: upGrad learner switches to career in ML & Al with 90% salary hike \n",
      "\n",
      "Processed headline:  upgrad learner switches to career in ml al with salary hike\n"
     ]
    }
   ],
   "source": [
    "# Process data['headlines] into clean_headline\n",
    "clean_headlines= []\n",
    "for headline in data['headlines']:\n",
    "    preprocessed_headlines = preprocess_sentence(headline, False)\n",
    "    clean_headlines.append(preprocessed_headlines)\n",
    "\n",
    "print(\"Original headline:\", data['headlines'][0], \"\\n\")\n",
    "print(\"Processed headline: \", clean_headlines[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aeed80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headlines    0\n",
      "text         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Apply onto the original dataset\n",
    "data['text'] = clean_text\n",
    "data['headlines'] = clean_headlines\n",
    "\n",
    "# Convert empty values into NaN\n",
    "data.replace('', np.nan, inplace=True)\n",
    "\n",
    "# Check if Nan exists\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Save the processed data\n",
    "data.to_csv('newssum_processed_text.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eacf68a4",
   "metadata": {},
   "source": [
    "### 2.3. Exclude lengthy sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99005d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>upgrad learner switches to career in ml al wit...</td>\n",
       "      <td>saurav kant alumnus upgrad iiit pg program mac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>delhi techie wins free food from swiggy for on...</td>\n",
       "      <td>kunal shah credit card bill payment platform c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new zealand end rohit sharma led india match w...</td>\n",
       "      <td>new zealand defeated india wickets fourth odi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aegon life iterm insurance plan helps customer...</td>\n",
       "      <td>aegon life iterm insurance plan customers enjo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>have known hirani for yrs what if metoo claims...</td>\n",
       "      <td>speaking sexual harassment allegations rajkuma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  \\\n",
       "0  upgrad learner switches to career in ml al wit...   \n",
       "1  delhi techie wins free food from swiggy for on...   \n",
       "2  new zealand end rohit sharma led india match w...   \n",
       "3  aegon life iterm insurance plan helps customer...   \n",
       "4  have known hirani for yrs what if metoo claims...   \n",
       "\n",
       "                                                text  \n",
       "0  saurav kant alumnus upgrad iiit pg program mac...  \n",
       "1  kunal shah credit card bill payment platform c...  \n",
       "2  new zealand defeated india wickets fourth odi ...  \n",
       "3  aegon life iterm insurance plan customers enjo...  \n",
       "4  speaking sexual harassment allegations rajkuma...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Check\n",
    "data = pd.read_csv(\"newssum_processed_text.csv\", index_col=[0])\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63e97b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE | number of NaN in 'text': 0\n",
      "AFTER | portion of NaN in 'text': 0.32%\n",
      "AFTER | portion of NaN in 'headlines' : 5.5%\n",
      "\n",
      "98379 92673\n"
     ]
    }
   ],
   "source": [
    "# Exclude text and headlines over the thresholds\n",
    "threshold_text = 45\n",
    "threshold_headline = 11\n",
    "\n",
    "data_temp = data.copy()\n",
    "print(f\"BEFORE | number of NaN in 'text': {data_temp['text'].isnull().sum()}\")\n",
    "\n",
    "# Make lengthy sentences NaN - text threshold = 45\n",
    "data_temp['text'] = data_temp['text'].apply(lambda x : np.nan if len(x.split()) > threshold_text else x)\n",
    "print(f\"AFTER | portion of NaN in 'text': {round(data_temp['text'].isnull().sum() / len(data) * 100, 2)}%\")\n",
    "\n",
    "# Make lengthy sentences NaN - headline threshold = 11\n",
    "data_temp['headlines'] = data_temp['headlines'].apply(lambda x : np.nan if len(x.split()) > threshold_headline else x)\n",
    "print(f\"AFTER | portion of NaN in 'headlines' : {round(data_temp['headlines'].isnull().sum() / len(data) * 100, 2)}%\\n\")\n",
    "\n",
    "# Drop all NaN\n",
    "data_temp.dropna(inplace=True)\n",
    "print(len(data), len(data_temp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a0855b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 92673 entries, 0 to 92672\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   headlines  92673 non-null  object\n",
      " 1   text       92673 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Assign 'data_temp' as 'data' again\n",
    "data = data_temp.reset_index(drop=True)\n",
    "data.info()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1dff1ace",
   "metadata": {},
   "source": [
    "### 2.4. Prepare inputs, outputs and Split into train & test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55b273a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "      <th>decoder_input</th>\n",
       "      <th>decoder_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>upgrad learner switches to career in ml al wit...</td>\n",
       "      <td>saurav kant alumnus upgrad iiit pg program mac...</td>\n",
       "      <td>sostoken upgrad learner switches to career in ...</td>\n",
       "      <td>upgrad learner switches to career in ml al wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>new zealand end rohit sharma led india match w...</td>\n",
       "      <td>new zealand defeated india wickets fourth odi ...</td>\n",
       "      <td>sostoken new zealand end rohit sharma led indi...</td>\n",
       "      <td>new zealand end rohit sharma led india match w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aegon life iterm insurance plan helps customer...</td>\n",
       "      <td>aegon life iterm insurance plan customers enjo...</td>\n",
       "      <td>sostoken aegon life iterm insurance plan helps...</td>\n",
       "      <td>aegon life iterm insurance plan helps customer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  \\\n",
       "0  upgrad learner switches to career in ml al wit...   \n",
       "1  new zealand end rohit sharma led india match w...   \n",
       "2  aegon life iterm insurance plan helps customer...   \n",
       "\n",
       "                                                text  \\\n",
       "0  saurav kant alumnus upgrad iiit pg program mac...   \n",
       "1  new zealand defeated india wickets fourth odi ...   \n",
       "2  aegon life iterm insurance plan customers enjo...   \n",
       "\n",
       "                                       decoder_input  \\\n",
       "0  sostoken upgrad learner switches to career in ...   \n",
       "1  sostoken new zealand end rohit sharma led indi...   \n",
       "2  sostoken aegon life iterm insurance plan helps...   \n",
       "\n",
       "                                      decoder_target  \n",
       "0  upgrad learner switches to career in ml al wit...  \n",
       "1  new zealand end rohit sharma led india match w...  \n",
       "2  aegon life iterm insurance plan helps customer...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add SOS and EOS tokens on headlines\n",
    "data['decoder_input'] = data['headlines'].apply(lambda x : 'sostoken '+ x)\n",
    "data['decoder_target'] = data['headlines'].apply(lambda x : x + ' eostoken')\n",
    "\n",
    "# Convert into np.array to make inputs and outputs\n",
    "encoder_input = np.array(data['text'])  # Input of encoder\n",
    "decoder_input = np.array(data['decoder_input']) # Input of decoder\n",
    "decoder_target = np.array(data['decoder_target']) # Label of decoder\n",
    "\n",
    "data.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01374c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of encoder input_train : 74138\n",
      "Number of decoder_input_train : 74138\n",
      "Number of encoder input_test : 18535\n",
      "Number of decoder input_test : 18535\n"
     ]
    }
   ],
   "source": [
    "# Use train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "encoder_input_train, encoder_input_test, decoder_input_train, decoder_input_test, decoder_target_train, decoder_target_test = \\\n",
    "train_test_split(encoder_input, decoder_input, decoder_target, test_size=0.2, random_state=20)\n",
    "\n",
    "print('Number of encoder input_train :', len(encoder_input_train))\n",
    "print('Number of decoder_input_train :', len(decoder_input_train))\n",
    "print('Number of encoder input_test :', len(encoder_input_test))\n",
    "print('Number of decoder input_test :', len(decoder_input_test))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc34dce7",
   "metadata": {},
   "source": [
    "### 2.5. Integer Encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea0bacfc",
   "metadata": {},
   "source": [
    "Text sequence converted into integer sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d18657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2562, 2998, 155, 312, 4679, 6716, 99, 506, 7853, 4863, 1847, 7090, 6149, 1, 210, 2, 14, 373, 3666, 48, 2382], [3, 13, 71, 38, 61, 4017, 182, 12, 5961, 600, 252, 3435, 437, 493, 2143, 29, 143, 275, 521, 722, 600, 8, 1, 38, 826, 2518, 5133, 581, 940, 461, 163], [16, 368, 1385, 1716, 352, 1953, 3236, 27, 295, 2519, 215, 1354, 3693, 72, 5651, 3109, 464, 5259, 223, 347, 6989, 1173, 639, 432, 1335, 150, 2519, 4018, 4018, 21, 1514, 2344, 6209, 1039, 9, 1963]]\n"
     ]
    }
   ],
   "source": [
    "# Text Sequence Tokenizer\n",
    "src_vocab = 8000 # Limit numbero of words: 8000\n",
    "src_tokenizer = Tokenizer(num_words=src_vocab) \n",
    "src_tokenizer.fit_on_texts(encoder_input_train) \n",
    "\n",
    "# Convert text sequence into integer sequence\n",
    "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
    "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
    "\n",
    "# Samples\n",
    "print(encoder_input_train[:3])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7f414c9",
   "metadata": {},
   "source": [
    "Headline sequence converted into integer sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e12602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input  [[1, 1405, 388, 206, 18, 712], [1, 20, 3, 98, 17, 1541, 10, 1780, 4], [1, 1781, 1995, 521, 3, 75, 17, 107, 231], [1, 93, 61, 121, 732, 3, 1375], [1, 71, 1932, 1003, 4, 138]]\n",
      "decoder  [[1405, 388, 206, 18, 712, 2], [20, 3, 98, 17, 1541, 10, 1780, 4, 2], [1781, 1995, 521, 3, 75, 17, 107, 231, 2], [93, 61, 121, 732, 3, 1375, 2], [71, 1932, 1003, 4, 138, 2]]\n"
     ]
    }
   ],
   "source": [
    "# Headline Sequence Tokenizer\n",
    "tar_vocab = 2000 # Limit numbero of words: 2000\n",
    "tar_tokenizer = Tokenizer(num_words=tar_vocab) \n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "# Convert headline sequence into integer sequence\n",
    "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train) \n",
    "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
    "\n",
    "# Samples\n",
    "print('input ',decoder_input_train[:5])\n",
    "print('decoder ',decoder_target_train[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bc8aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding - apply pad_sequence\n",
    "encoder_input_train = pad_sequences(encoder_input_train, maxlen=threshold_text, padding='post')\n",
    "decoder_input_train = pad_sequences(decoder_input_train, maxlen=threshold_headline, padding='post')\n",
    "decoder_target_train = pad_sequences(decoder_target_train, maxlen=threshold_headline, padding='post')\n",
    "\n",
    "encoder_input_test = pad_sequences(encoder_input_test, maxlen=threshold_text, padding='post')\n",
    "decoder_input_test = pad_sequences(decoder_input_test, maxlen=threshold_headline, padding='post')\n",
    "decoder_target_test = pad_sequences(decoder_target_test, maxlen=threshold_headline, padding='post')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4c7db5f",
   "metadata": {},
   "source": [
    "## 3. Abstract Summarization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f12df6f2",
   "metadata": {},
   "source": [
    "### 3.1. Build Model with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b533310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# Setting Parameters\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "# Encoder - Input Layer\n",
    "encoder_inputs = Input(shape=(threshold_text,))\n",
    "\n",
    "# Encoder - Embedding Layer\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# Encoder - LSTM1\n",
    "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# Encoder - LSTM2\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout = 0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# Encoder - LSTM3\n",
    "encoder_lstm3 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout = 0.4)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm3(encoder_output2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c9dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder - Input Layer\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# Decoder - Embedding Layer\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# Decoder - LSTM\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "# Decoer - Output Layer\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c1e6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 45)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 45, 128)      1024000     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 45, 256), (N 394240      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   [(None, 45, 256), (N 525312      lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 128)    256000      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   [(None, 45, 256), (N 525312      lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   [(None, None, 256),  394240      embedding_3[0][0]                \n",
      "                                                                 lstm_6[0][1]                     \n",
      "                                                                 lstm_6[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 2000)   514000      lstm_7[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,633,104\n",
      "Trainable params: 3,633,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Definition\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55a53f3b",
   "metadata": {},
   "source": [
    "### 3.2. Apply Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1a6132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 45)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 45, 128)      1024000     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 45, 256), (N 394240      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   [(None, 45, 256), (N 525312      lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 128)    256000      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   [(None, 45, 256), (N 525312      lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   [(None, None, 256),  394240      embedding_3[0][0]                \n",
      "                                                                 lstm_6[0][1]                     \n",
      "                                                                 lstm_6[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AdditiveAttent (None, None, 256)    256         lstm_7[0][0]                     \n",
      "                                                                 lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 512)    0           lstm_7[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 2000)   1026000     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,145,360\n",
      "Trainable params: 4,145,360\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import AdditiveAttention\n",
    "\n",
    "# Attention Layer\n",
    "attn_layer = AdditiveAttention(name='attention_layer')\n",
    "\n",
    "# Convey all hidden state to attention layer \n",
    "attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
    "\n",
    "# Connect the result of attention and hidden_state\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# Decoder - Output Layer\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# Model Definition\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e8ad90b",
   "metadata": {},
   "source": [
    "### 3.3. Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1e637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "\n",
    "# Fit and get history\n",
    "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test), \\\n",
    "          batch_size=256, callbacks=[es], epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd1e267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "\n",
    "plt.title('Train & Test Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "742c7060",
   "metadata": {},
   "source": [
    "### Loss Analysis\n",
    "Both train and test loss derecaes as the fit goes on, early-stopped at 24th epoch as validation loss increased for two epochs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b04bb4d",
   "metadata": {},
   "source": [
    "## 4. Compare Sentences: Inferred and Original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0dcb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer → word from 'text' set\n",
    "src_index_to_word = src_tokenizer.index_word\n",
    "\n",
    "# Word → integer from 'headlines' set\n",
    "tar_word_to_index = tar_tokenizer.word_index \n",
    "\n",
    "# Integer → word from 'headlines' set\n",
    "tar_index_to_word = tar_tokenizer.index_word \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705718e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build encoder\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder - Tensor indicating states of 1 time-step ahead\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "# Decoder - Embedding Layer\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# Decoder - LSTM preserve and use state_h and state_c\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7bd5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder - Attention Layer (hidden state layer)\n",
    "decoder_hidden_state_input = Input(shape=(threshold_text, hidden_size))\n",
    "attn_out_inf = attn_layer([decoder_outputs2, decoder_hidden_state_input])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# Decoder - Output Layer\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
    "\n",
    "# Decoder Result\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440f80e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # get state of encoder from the sequence\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS token>\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition: # Repeat loop until stop_condition == True\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = tar_index_to_word[sampled_token_index]\n",
    "\n",
    "        if (sampled_token!='eostoken'):\n",
    "            decoded_sentence += ' ' + sampled_token\n",
    "\n",
    "        #  <EOS token> or length_limit == STOP!\n",
    "        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (threshold_headline-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (length ==1)\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update the state\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7afcc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integer Seq → Word Seq ('text')\n",
    "def seq2text(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if (i!=0):\n",
    "            temp = temp + src_index_to_word[i]+' '\n",
    "    return temp\n",
    "\n",
    "# Integer Seq → Word Seq ('headlines')\n",
    "def seq2summary(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if i not in [0, 1, 2]:\n",
    "            temp = temp + tar_index_to_word[i]+' '\n",
    "    return temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ba7a68",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:  cbi filed chargesheet kanpur gst commissioner chand bribery case chand first senior officer gst department established last year face corruption charges agency officials said cbi also charged wife others corruption criminal conspiracy among others  \n",
      "\n",
      "Original headlines:  gst in case \n",
      "Predicted headlines:   cbi officers booked for corruption of yr old\n",
      "\n",
      "\n",
      "Original text:  actor vivek revealed lived used public toilets get role film company director ram gopal varma found good looking part added finally met kept legs said yeh kya hai convinced  \n",
      "\n",
      "Original headlines:  in used public toilet to get company role \n",
      "Predicted headlines:   people are like to be made\n",
      "\n",
      "\n",
      "Original text:  us friday announced largest ever sanctions north korea claimed president donald trump sanctions target vessels shipping companies trade businesses north korea sanctions us imposing sanctions put pressure north korea nuclear ballistic missile programme  \n",
      "\n",
      "Original headlines:  us announces largest ever sanctions against north korea \n",
      "Predicted headlines:   us imposes sanctions on north korea sanctions\n",
      "\n",
      "\n",
      "Original text:  jewish couple month old daughter removed american airlines flight airline told passengers complained body american airlines claimed given hotel room meal night booked onto new flight man said believes removed religious reasons  \n",
      "\n",
      "Original headlines:  couple month old daughter removed from plane over body \n",
      "Predicted headlines:   couple on flight after being off flight\n",
      "\n",
      "\n",
      "Original text:  book albert einstein tour asia claimed scientist described chinese children however according author einstein called japanese intellectual needs nation seem weaker ones  \n",
      "\n",
      "Original headlines:  called chinese people in his \n",
      "Predicted headlines:   indian woman claims it is\n",
      "\n",
      "\n",
      "Original text:  union road minister nitin gadkari reportedly ran man claiming officer special duty nagpur airport man demanded front row seats access vip airport exposed gadkari walked assistant informed officer  \n",
      "\n",
      "Original headlines:  gadkari runs into man as his officer on special duty \n",
      "Predicted headlines:   minister gadkari man to airport\n",
      "\n",
      "\n",
      "Original text:  india born professor institute technology named dean university school engineering electrical engineering computer science mit largest academic department start tenure july born chennai moved usa high school  \n",
      "\n",
      "Original headlines:  india born named of mit school of \n",
      "Predicted headlines:   india born to be for\n",
      "\n",
      "\n",
      "Original text:  sacked aap minister kapil mishra sunday said arvind kejriwal led delhi government crore public money government allocated amount various departments nothing spent major projects related education health water added biggest governance failure history india mishra claimed  \n",
      "\n",
      "Original headlines:  delhi govt public funds worth cr kapil \n",
      "Predicted headlines:   aap govt to cr to kejriwal\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare target headlines and predicted headlines\n",
    "for i in range(0,8):\n",
    "    print(\"Original text: \", seq2text(encoder_input_test[i]), \"\\n\")\n",
    "    print(\"Original headlines: \", seq2summary(decoder_input_test[i]))\n",
    "    print(\"Predicted headlines: \", decode_sequence(encoder_input_test[i].reshape(1, threshold_text)))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd9312ff",
   "metadata": {},
   "source": [
    "## 5. Try Extract Summarization with `Summa`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7dee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library\n",
    "from summa.summarizer import summarize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3ff4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original data (not processed) to pick a sample\n",
    "data = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')\n",
    "data.head(30)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d57f796",
   "metadata": {},
   "source": [
    "Going to use the sentece below: \n",
    "\n",
    "```\n",
    "Original text:  us friday announced largest ever sanctions north korea claimed president donald trump sanctions target vessels shipping companies trade businesses north korea sanctions us imposing sanctions put pressure north korea nuclear ballistic missile programme  \n",
    "\n",
    "Original headlines:  us announces largest ever sanctions against north korea \n",
    "Predicted headlines:   us imposes sanctions on north korea sanctions\n",
    "```\n",
    "\n",
    "Find the original text from original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b010364b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Indian Oil looking for annual deal to buy crud...</td>\n",
       "      <td>Indian Oil Corporation on Wednesday said it's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>US imposes sanctions on Venezuelan state-owned...</td>\n",
       "      <td>The US on Monday imposed sanctions on Venezuel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>US charges Huawei with stealing trade secrets,...</td>\n",
       "      <td>The US has charged Huawei and its CFO Meng Wan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>Centre approves Ã¢ÂÂ¹7,000 cr disaster relief...</td>\n",
       "      <td>The Centre on Tuesday approved additional Cent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Nearly 4,500 cases pending per High Court judg...</td>\n",
       "      <td>According to the National Judicial Data Grid, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             headlines  \\\n",
       "24   Indian Oil looking for annual deal to buy crud...   \n",
       "210  US imposes sanctions on Venezuelan state-owned...   \n",
       "255  US charges Huawei with stealing trade secrets,...   \n",
       "292  Centre approves Ã¢ÂÂ¹7,000 cr disaster relief...   \n",
       "498  Nearly 4,500 cases pending per High Court judg...   \n",
       "\n",
       "                                                  text  \n",
       "24   Indian Oil Corporation on Wednesday said it's ...  \n",
       "210  The US on Monday imposed sanctions on Venezuel...  \n",
       "255  The US has charged Huawei and its CFO Meng Wan...  \n",
       "292  The Centre on Tuesday approved additional Cent...  \n",
       "498  According to the National Judicial Data Grid, ...  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the word 'Sanction'\n",
    "data_sanctions = data[data.apply(lambda row: row.astype(str).str.contains('sanction').any(), axis=1) == True]\n",
    "data_sanctions.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae65268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17033</th>\n",
       "      <td>Russia 'cheating' on North Korea sanctions: US...</td>\n",
       "      <td>Addressing the Security Council on Monday, US ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29520</th>\n",
       "      <td>Iran lists demands to stay in nuclear deal</td>\n",
       "      <td>Iran's Supreme Leader Ayatollah Ali Khamenei h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31019</th>\n",
       "      <td>Send dismantled nukes to US for easing sanctio...</td>\n",
       "      <td>US' National Security Advisor John Bolton has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31641</th>\n",
       "      <td>'Maximum pressure' against North Korea worked: US</td>\n",
       "      <td>The \"maximum pressure\" campaign launched by US...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33501</th>\n",
       "      <td>Australia to monitor North Korean ships' 'illi...</td>\n",
       "      <td>Australia will send military aircraft to the J...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               headlines  \\\n",
       "17033  Russia 'cheating' on North Korea sanctions: US...   \n",
       "29520         Iran lists demands to stay in nuclear deal   \n",
       "31019  Send dismantled nukes to US for easing sanctio...   \n",
       "31641  'Maximum pressure' against North Korea worked: US   \n",
       "33501  Australia to monitor North Korean ships' 'illi...   \n",
       "\n",
       "                                                    text  \n",
       "17033  Addressing the Security Council on Monday, US ...  \n",
       "29520  Iran's Supreme Leader Ayatollah Ali Khamenei h...  \n",
       "31019  US' National Security Advisor John Bolton has ...  \n",
       "31641  The \"maximum pressure\" campaign launched by US...  \n",
       "33501  Australia will send military aircraft to the J...  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the word 'ballistic'\n",
    "data_sanctions_ballistic =  data_sanctions[data_sanctions.apply(lambda row: row.astype(str).str.contains('ballistic').any(), axis=1) == True]\n",
    "data_sanctions_ballistic.head()\n",
    "\n",
    "# Found the index == 46444\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09c9404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The US on Friday announced the largest-ever sanctions on North Korea as claimed by President Donald Trump. The sanctions will target more than 50 vessels, shipping companies, and trade businesses assisting North Korea in evading sanctions. The US has been imposing sanctions to put pressure on North Korea over its nuclear and ballistic missile programme.'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign the text on variable 'text'\n",
    "text = data['text'][46444]\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbab576d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary with Extraction:\n",
      "\n",
      "The US has been imposing sanctions to put pressure on North Korea over its nuclear and ballistic missile programme.\n"
     ]
    }
   ],
   "source": [
    "# Test Extract Summarization\n",
    "print('Summary with Extraction:\\n')\n",
    "print(summarize(text, ratio=0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e0440a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary with Abstraction:\n",
      "\n",
      " us imposes sanctions on north korea sanctions\n"
     ]
    }
   ],
   "source": [
    "# Result of Abstract Summarization\n",
    "print(\"Summary with Abstraction:\\n\")\n",
    "print(decode_sequence(encoder_input_test[2].reshape(1, threshold_text)))\n",
    "      "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "645ef388",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "Extract Summarization has more grammatical sentences, and longer than Abstract summary.  \n",
    "Abstract Summarization uses more general words. Some text doesn't make sense.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

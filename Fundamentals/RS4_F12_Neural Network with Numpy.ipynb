{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental 12. Neural Network with Numpy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MNIST 데이터를 로드. 다운로드하지 않았다면 다운로드까지 자동으로 진행됩니다. \n",
    "mnist = keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "# 모델에 맞게 데이터 가공\n",
    "x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0\n",
    "x_train_reshaped = x_train_norm.reshape(-1, x_train_norm.shape[1]*x_train_norm.shape[2])\n",
    "x_test_reshaped = x_test_norm.reshape(-1, x_test_norm.shape[1]*x_test_norm.shape[2])\n",
    "\n",
    "print(x_train_norm.shape)\n",
    "print(x_train_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 50)                39250     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                510       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 39,760\n",
      "Trainable params: 39,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 2s 644us/step - loss: 0.5024 - accuracy: 0.8805\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 1s 623us/step - loss: 0.2356 - accuracy: 0.9330\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 1s 625us/step - loss: 0.1841 - accuracy: 0.9481\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 1s 641us/step - loss: 0.1529 - accuracy: 0.9568\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 1s 633us/step - loss: 0.1313 - accuracy: 0.9628\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 1s 628us/step - loss: 0.1151 - accuracy: 0.9678\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 1s 632us/step - loss: 0.1024 - accuracy: 0.9716\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 1s 627us/step - loss: 0.0920 - accuracy: 0.9743\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 1s 629us/step - loss: 0.0835 - accuracy: 0.9771\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 1s 634us/step - loss: 0.0763 - accuracy: 0.9790\n",
      "313/313 - 0s - loss: 0.1067 - accuracy: 0.9694 - 228ms/epoch - 727us/step\n",
      "test_loss: 0.10673899948596954 \n",
      "test_accuracy: 0.9693999886512756\n"
     ]
    }
   ],
   "source": [
    "# 딥러닝 모델 구성 - 2 Layer Perceptron\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(50, activation='sigmoid', input_shape=(784,)))  # 입력층 d=784, 은닉층 레이어 H=50\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))   # 출력층 레이어 K=10\n",
    "model.summary()\n",
    "# 같은 Dense Layer인데 어떻게 입력층, 은닉층, 출력층을 구분할 수 있지? \n",
    "\n",
    "\n",
    "# 모델 구성과 학습\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.fit(x_train_reshaped, y_train, epochs=10)\n",
    "\n",
    "# 모델 테스트 결과\n",
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped,y_test, verbose=2)\n",
    "print(f\"test_loss: {test_loss} \")\n",
    "print(f\"test_accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "# 입력층 데이터의 모양(shape)\n",
    "print(x_train_reshaped.shape)\n",
    "\n",
    "# 테스트를 위해 x_train_reshaped의 앞 5개의 데이터를 가져온다.\n",
    "X = x_train_reshaped[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1.shape: (784, 50)\n",
      "X.shape: (5, 784)\n",
      "b1.shape: (50,)\n",
      "a1.shape: (5, 50)\n"
     ]
    }
   ],
   "source": [
    "weight_init_std = 0.1\n",
    "input_size = 784\n",
    "hidden_size= 50\n",
    "\n",
    "# 인접 레이어간 관계를 나타내는 파라미터 W를 생성하고 random 초기화\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "  \n",
    "# 바이어스 파라미터 b를 생성하고 Zero로 초기화\n",
    "b1 = np.zeros(hidden_size)\n",
    "\n",
    "a1 = np.dot(X, W1) + b1   # 은닉층 출력\n",
    "\n",
    "print(f'W1.shape:',W1.shape)\n",
    "print(f'X.shape:',X.shape)\n",
    "print(f'b1.shape:', b1.shape)\n",
    "print(f'a1.shape:',a1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.56461856 0.79461962 0.66870849 0.17687708 0.76846896 0.49497569\n",
      " 0.66528396 0.68346984 0.28668059 0.30015249 0.53416296 0.68047229\n",
      " 0.34497634 0.57376959 0.56891437 0.31587699 0.62001097 0.1365387\n",
      " 0.79219017 0.45517714 0.48363751 0.37195131 0.60081537 0.65676669\n",
      " 0.17546774 0.71483481 0.36758448 0.68615235 0.29571123 0.39704454\n",
      " 0.49268748 0.37345993 0.6071123  0.59631507 0.87206135 0.74068702\n",
      " 0.61607596 0.52530182 0.85723349 0.56424048 0.65571605 0.69369721\n",
      " 0.63686373 0.39145667 0.74066986 0.41706083 0.64859833 0.22969431\n",
      " 0.19363631 0.49705971]\n"
     ]
    }
   ],
   "source": [
    "# 위 수식의 sigmoid 함수를 구현해 봅니다.\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))  \n",
    "\n",
    "\n",
    "z1 = sigmoid(a1)\n",
    "print(z1[0])  # sigmoid의 출력은 모든 element가 0에서 1사이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "푸슝~3\n"
     ]
    }
   ],
   "source": [
    "# 단일 레이어 구현 함수\n",
    "def affine_layer_forward(X, W, b):\n",
    "    y = np.dot(X, W) + b\n",
    "    cache = (X, W, b)\n",
    "    return y, cache\n",
    "\n",
    "print('푸슝~3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.55209304 -0.32546467 -0.13177823 -0.76429516 -0.25845149  0.22373798\n",
      " -0.67335654 -0.00798276  0.28986214 -0.72391905]\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_size = 50\n",
    "output_size = 10\n",
    "\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "z1 = sigmoid(a1)\n",
    "a2, cache2 = affine_layer_forward(z1, W2, b2)    # z1이 다시 두번째 레이어의 입력이 됩니다. \n",
    "\n",
    "print(a2[0])  # 최종 출력이 output_size만큼의 벡터가 되었습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 활성화함수 (Activation Function)\n",
    "\n",
    "- 딥러닝의 입력층, 은닉층, 출력층은 기본적으로 선형함수  \n",
    "- 선형함수에 또 다른 선형함수를 적용한 결과는 선형이어서, 층을 여러개 쌓는 의미가 옅어짐  \n",
    "- 각 층 사이에 비선형함수를 두어 데이터의 표현력을 좋게 하기 위해 사용됨 \n",
    "\n",
    "#### 1. Sigmoid 함수\n",
    "\n",
    "모든 출력이 0에서 1 사이로 고정되는 함수  \n",
    "신경망 초기에는 많이 활용되었으나 현재는 거의 활용되지 않는다.  \n",
    "x=0일 때 `y = 1/2x`이므로 미분 최대값은 1/4, x값이 커질수록 0에 수렴하게 됨  \n",
    "BackPropagation 시에 미분값이 소실될 가능성이 높다. \n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1 + \\exp^-x}$\n",
    "\n",
    "#### 2. Tanh 함수 (Hyperbolic tangent 함수)\n",
    "\n",
    "모든 출력이 -1에서 1 사이로 고정되는 함수. 함수의 중심값을 0으로 옮겨 최적화 과정에서 느려지는 문제를 해결\n",
    "\n",
    "$tanh(x) = \\frac{e^x - e^-x}{e^x + e^-x}$\n",
    "\n",
    "#### 3. ReLU (Rectified Linear Unit)\n",
    "\n",
    "x > 0일 때는 기울기가 1인 직선, 음수일 때는 함수값이 0이 된다.  \n",
    "sigmoid, tanh 함수와 비교 시 학습이 훨씬 빨라진다.  \n",
    "x < 0인 값에 대해 뉴런이 죽을 수 있다는 단점이 있다.  \n",
    "\n",
    "\n",
    "#### 4. softmax 함수 \n",
    "\n",
    "입력받은 값을 출력으로 0~1 사이의 값으로 모두 정규화  \n",
    "출력값들의 총합은 항상 1이 되는 특성을 가진 함수  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 손실함수 (Loss Function or Cost Function)\n",
    "\n",
    "#### 평균제곱오차 (Mean Square Error)\n",
    "\n",
    "#### 교차 엔트로피 (Cross Entropy)\n",
    "두 확률분포 사이의 유사도가 클수록 작아지는 값.  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법 (Gradient Descent)\n",
    "\n",
    "딥러닝 모델 최적화 방안 중 하나. 손실함수의 결과를 최소화하는 parameter를 찾는 과정이다.  \n",
    "\n",
    "가중치를 손실함수에 대입한 값이 최소화되어야 함. \n",
    "\n",
    "목표 함수인 손실 함수를 비용함수 w에 대해 편미분하고, 학습률과 곱한 값을 처음 선정한 가중치에서 뺌. \n",
    "\n",
    "여러 step을 걸쳐 손실함수의 값이 거의 변하지 않을 때까지 진행함.  \n",
    "\n",
    "값이 가장 낮은 지점으로 경사를 타고 하강하는 기법"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

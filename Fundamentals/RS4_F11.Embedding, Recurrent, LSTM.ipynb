{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental 11. Embedding, Recurrent, LSTM\n",
    "\n",
    "### Embedding Layer\n",
    "\n",
    "#### Distributed Representation (분산 표현)\n",
    "**\"유사한 맥락에서 나타나는 단어는 그 의미도 비슷하다\"** 는 가설을 기반으로, 모든 단어를 고정 차원의 벡터로 표현하는 것  \n",
    "유사한 맥락에서 나타난 단어는 거리가 가까운 벡터로, 그렇지 않은 단어는 거리가 먼 벡터로 조정  \n",
    "\n",
    "#### Sparse Represention (희소 표현)\n",
    "단어 혹은 단어의 의미를 특정 차원의 벡터에 직접 Mapping 하는 방식\n",
    "\n",
    "### 잘 모르지만 일단 기록\n",
    "\n",
    "\"딥러닝은 미분을 기반으로 동작한다. Embedding Layer는 단어를 대응시켜줄 뿐이니 미분이 불가능하다.  \n",
    "신경망 설계를 할 때, 연산 결과를 Embedding Layer에 연결하는 것은 불가능하다.  \n",
    "다시, Embedding Layer는 입력에 직접 연결되게 사용해야 한다.\" \n",
    "\n",
    "### Recurrent Layer\n",
    "\n",
    "자연어는 `Sequence Data`, 순차적인 동시에 요소 간 연관성이 있는 데이터   \n",
    "입력 단어 이전에 어떤 단어가 있었는 지를 확인하려면?  \n",
    "\n",
    "- 해당 Time-step의 input에 이전 Time-step의 `INPUT`을 추가할 것인지  \n",
    "- 해당 Time-step의 input에 이전 Time-step의 `HIDDEN LAYER` 정보를 추가할 것인지. \n",
    "\n",
    "(입력 차원, 출력 차원)이라는 한 Weight를 순차적으로 갱신하는 것이 RNN\n",
    "\n",
    "### LSTM (Long Short-Term Memory) Layer\n",
    "\n",
    "기울기 소실 문제를 해결하기 위해 고안된 RNN 레이어.  \n",
    "`Forget Gate`, `Input Gate`, `Output Gate`로 구성된다.\n",
    "\n",
    "문장 하나에 대해 단어 단위로 기록을 업데이트하는 메모장이 있다고 가정하자면,  \n",
    "\n",
    "`forget gate`는 이전 단어까지에서 필요 없는 정보를 지우는 역할을 수행 (이전 단계의 cell state 정보 수정 gate)  \n",
    "`input gate`는 이번 단어에서 꼭 필요한 정보를 추가하는 역할을 수행 (새로운 정보를 cell state에 얼마나 반영할 지 결정)  \n",
    "`output gate`는 메모한 내용을 참고해 기억의 왜곡을 줄이는 역할 (새롭게 만들어진 Cell state를 새로운 Hidden state에 얼마나 반영할 지 결정)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
